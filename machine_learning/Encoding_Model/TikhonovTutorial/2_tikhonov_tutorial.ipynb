{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Encoding Models with Word Embeddings\n",
    "for NeuroHackademy 2020, by Alex Huth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some basic stuff we'll need later\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Regularized (Ridge) Regression\n",
    "\n",
    "Remember when we talked about the noise term, $\\epsilon(t)$? We said it was any part of $y(t)$ that couldn't be predicted from $x(t)$. We can write this mathematically as the difference between $y(t)$ and the predicted value based on $x(t)$ and our estimated weights (this comes from re-arranging the equation listed under \"Define the model\" above):\n",
    "\n",
    "$$\\epsilon(t) = y(t) - x(t) \\beta_{OLS}$$\n",
    "\n",
    "Huh. When we write it this way, it looks a lot like the loss function $\\mathcal{L}(\\beta)$, doesn't it? In fact, the loss function is exactly the sum of the squared errors. This means our OLS regression model made the assumption that _$\\epsilon(t)$ is as small as possible_, because we selected $\\beta_{OLS}$ to minimize the size of the loss.\n",
    "\n",
    "This all seems sensible in some situations. We don't know a priori which parts of $y(t)$ are signal (i.e. predictable from $x(t)$) and which parts are noise, so why not try to predict as much as possible? Well, it's time for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-family:Snell Roundhand, cursive; font-size: 24pt\">**A CAUTIONARY TALE**</span>.\n",
    "\n",
    "<span style=\"color:red; font-style:italic\">\n",
    "A young neuroscientist is performing an experiment.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:red; font-style:italic\">\n",
    "\"I just scanned a brain, and my scanning machine gave me big lists of numbers,\" they said, \"I _know_ that the data from the scanning machine is always noisy, but I'm just going to try to fit a model that makes my total error small. What could be finer than finding small error?\"\n",
    "<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cautionary_data = np.load(\"/Users/clmnlab/Downloads/data/we_word_embeddings/a_cautionary_tale.npz\")\n",
    "\n",
    "X_train = cautionary_data[\"X_train\"]\n",
    "X_test = cautionary_data[\"X_test\"]\n",
    "Y_train = cautionary_data[\"Y_train\"]\n",
    "Y_test = cautionary_data[\"Y_test\"]\n",
    "beta_true = cautionary_data[\"beta_true\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 98), (25, 98), (100,), (25,), (98,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape, beta_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-style:italic\">\n",
    "The young neuroscientist diligently fired up their Jupyter notebook and ran an ordinary least squares regression using the noisy data. At first, the result was amazing! The loss was tiny! The model explained 99.9% of the variance in the training dataset! This seemed like the breakthrough that the young neuroscientist had been waiting for.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.5866841305830968\n",
      "training R^2: 0.9997128446796295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/sampark/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "beta_estimate = np.linalg.lstsq(X_train, Y_train)[0]\n",
    "\n",
    "training_loss = ((X_train.dot(beta_estimate) - Y_train)**2).sum()\n",
    "print(\"training loss:\", training_loss)\n",
    "print(\"training R^2:\", 1 - training_loss / (Y_train**2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-style:italic\">\n",
    "But upon further reflection, the young neuroscientist realized that something was amiss. The result was not as good as it seemed to be. The young neuroscientist exclaimed, \"What's this? Can it be? My regression weights make no sense! Some values are huge, some values are tiny, and if I try to predict on held-out test data, the result is abhorrent! Oh, woe is me! Why did I try to exactly minimize error?? _WHY??_\"\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated beta: [ -4594.58174457  -5794.05369566 -16106.83482789   2537.96474564\n",
      " -20926.22612195   6882.38422856   1947.54105308  -8592.97158243\n",
      "   1881.4021397   14220.50340837   7104.77141908   2674.25812034\n",
      "   -556.8779624   -5135.58455923  25405.97398904 -12923.10730094\n",
      "   7244.48361127 -15465.88839356  23843.98434501   5982.93541493\n",
      "    657.83643116    303.09282762   4328.59634515   -720.03371849\n",
      "  -6852.75144497  -5656.20832979   9576.27308336    755.25890776\n",
      " -11696.10420832   2533.25589156   9624.41909331  11002.24874853\n",
      "  -1752.55711419  -4369.5049089  -10505.11711087  12208.6203449\n",
      "   5706.6515336     620.95407088   2800.01839107    781.33987599\n",
      "  12661.99278062  -1137.71995329 -16021.34548968   -834.93342626\n",
      "   -404.27049909    514.11979727   9724.52766708  16061.11955206\n",
      "    969.98874988   4597.33134751   5796.7067284   16107.60840224\n",
      "  -2538.85103563  20926.72555763  -6885.37956355  -1947.84711155\n",
      "   8596.0629129   -1880.72199485 -14219.07811462  -7108.85598295\n",
      "  -2673.14110701    560.15034711   5138.52794271 -25404.9666643\n",
      "  12922.98673293  -7244.84117473  15463.89439615 -23844.2385175\n",
      "  -5983.91415484   -655.94080304   -304.08486003  -4328.29767032\n",
      "    718.67496481   6852.97697615   5656.38143551  -9576.10194558\n",
      "   -755.06424912  11695.26704979  -2533.95463328  -9625.81607722\n",
      " -11000.41607626   1748.43271093   4370.90626439  10504.74037417\n",
      " -12207.80187654  -5707.76014116   -621.9863826   -2797.41776713\n",
      "   -783.98209074 -12664.42368809   1139.28738176  16023.3942347\n",
      "    836.33525096    402.25886819   -513.81172902  -9726.48408825\n",
      " -16061.69357997   -968.86593289]\n",
      "Correlation between true & predicted Y_test: 0.039219441088034664\n"
     ]
    }
   ],
   "source": [
    "print(\"estimated beta:\", beta_estimate)\n",
    "\n",
    "Y_test_pred = X_test.dot(beta_estimate)\n",
    "print(\"Correlation between true & predicted Y_test:\", np.corrcoef(Y_test, Y_test_pred)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this sorry story has convinced you that perfectly minimizing error can cause problems. And the alternative viewpoint just seems like common sense: if you _know_ that your data are noisy, then it just doesn't make sense to make your noise term as small as possible--especially if \"as small as possible\" might mean that you could have _zero noise_.\n",
    "\n",
    "_(Aside: under what situations would you expect the OLS loss to become exactly zero?)_\n",
    "\n",
    "The solution to this problem is **regularization**, which is a family of techniques for adding extra constraints to regression models that balance against simply minimizing loss. The particular technique that we will discuss in detail today is called **ridge regression** or [**Tikhonov regularization**](https://en.wikipedia.org/wiki/Tikhonov_regularization).\n",
    "\n",
    "### Intuition behind ridge regression\n",
    "The problem with OLS regression is that it is _overconfident_ in its estimation of the weights $\\beta_{OLS}$. While attempting to minimize loss, OLS will allow the weights to take on any values, even extreme and bizarre values that are incredibly unlikely to accurately reflect reality. Ridge regression solves this problem by forcing the values in $\\beta$ to be small (i.e. close to zero).\n",
    "\n",
    "This is roughly equivalent to assuming that the noise term, $\\epsilon(t)$, is large (or, at least, larger than it would have otherwise been). You can see this by thinking about the equation for the noise term: $\\epsilon(t) = y(t) - x(t) \\beta$. We know that $\\epsilon(t)$ really shouldn't be any bigger than $y(t)$--worst case we can set $\\beta=0$ and then we get an exact equivalence. If weights $\\beta_{OLS}$ exactly minimize the size of the noise term, then making the weights smaller will necessarily make the noise term bigger.\n",
    "\n",
    "### Ridge regression\n",
    "The simplest way to describe ridge regression mathematically is including a **penalty** on the size of the weights in the loss function. Specifically, ridge regression penalizes the sum of the squared weights, leading to a new and improved loss function that we'll call $\\mathcal{L}_{ridge}(\\beta)$:\n",
    "\n",
    "$$ \\mathcal{L}_{ridge}(\\beta) = \\sum_{t=1}^T (y(t) - x(t) \\beta)^2 + \\lambda \\sum_{i=1}^p \\beta_i^2 $$\n",
    "\n",
    "or, in fancy linear algebra terms:\n",
    "\n",
    "$$ \\mathcal{L}_{ridge}(\\beta) = (Y - X\\beta)^\\top (Y - X \\beta) + \\lambda \\beta^\\top \\beta $$\n",
    "\n",
    "The first term on the right hand side of this equation is the same squared error loss that we used before for OLS. The second term is the sum of the squares of all the weights in $\\beta$, multiplied by a scalar variable $\\lambda$ that we will call the **ridge coefficient**.\n",
    "\n",
    "The ridge coefficient $\\lambda$ determines the _strength_ of the regularization that's applied in ridge regression: \n",
    "* If you give $\\lambda$ a large value, then the penalty term will be big relative to the loss, and the resulting weights will be very small. (In the limit of very large $\\lambda$ you will force the weights to be almost exactly zero!) \n",
    "* If you give $\\lambda$ a small value, then the penalty term will be small relative to the loss, and the resulting weights will not be too different from the OLS weights. (In the limit of $\\lambda \\rightarrow 0$, the penalty term will be zero and you'll get back exactly the OLS solution!)\n",
    "\n",
    "To get the ridge regression weights, $\\beta_{ridge}$, you minimize the ridge loss function. We don't need to go through the full derivation of the solution (though it's pretty fun, and easy to do based on the matrix calculus we did for the OLS solution!), so let's just take a look at the answer:\n",
    "\n",
    "$$ \\beta_{ridge} = (X^\\top X + \\lambda I)^{-1} X^\\top Y $$\n",
    "\n",
    "The only difference between this and the previous $\\beta_{OLS}$ solution is that we added the factor $\\lambda I$ (where $I$ is the identity matrix, the linear algebra equivalent of the number 1) inside the matrix inverse. Pretty simple! \n",
    "\n",
    "Let's try it out on the data from our cautionary tale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start by picking a lambda. THIS IS NOT HOW THIS IS USUALLY DONE! This is just illustrative\n",
    "ridge_lambda = 10.0\n",
    "\n",
    "# then we'll compute the ridge weights using exactly the formula listed above\n",
    "# (as before, this is not used in practice. It's slow and numerically unstable. But simple and clear!)\n",
    "beta_estimate_ridge = np.linalg.inv(X_train.T.dot(X_train) + ridge_lambda * np.eye(98)).dot(X_train.T).dot(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge estimated beta: [ 1.20284592  0.54358022 -0.17076853 -0.46913852  0.04712979 -0.43252916\n",
      " -0.46940385  0.75993806 -0.66024964  0.05161744 -1.28231356  0.14639374\n",
      "  1.08433341  1.07531835  0.15202215 -0.43115172  0.42039943  0.19601658\n",
      " -0.25279569 -0.67608631  0.75461698 -0.36126768  0.37316565 -0.57848406\n",
      " -0.29988382 -0.28430522  0.29505655  0.19701606 -0.43751896 -0.30376373\n",
      " -0.42344552  0.34901169 -1.70650378  0.27316785 -0.42136402  0.20792123\n",
      " -0.36047633 -0.72054648  1.11409482 -0.31659404 -0.47916971  0.26895738\n",
      "  0.54207873  0.8692063  -0.00460897 -0.40236788 -1.09188709 -0.45186825\n",
      "  0.88325812  1.20273579  0.54385658 -0.1706784  -0.46920649  0.04702301\n",
      " -0.43253862 -0.46943782  0.75993798 -0.66039318  0.05155487 -1.28224771\n",
      "  0.14634859  1.08420573  1.07530732  0.15183459 -0.43115928  0.42039769\n",
      "  0.19616878 -0.25263899 -0.67629854  0.75444664 -0.36124037  0.37313815\n",
      " -0.57852429 -0.30003918 -0.28429678  0.29506545  0.19694568 -0.4374332\n",
      " -0.30351455 -0.42354956  0.34898442 -1.7065731   0.27308578 -0.42114397\n",
      "  0.20804817 -0.36047132 -0.72055885  1.11412975 -0.31670748 -0.4791946\n",
      "  0.26896252  0.54195274  0.86903458 -0.00458904 -0.4023907  -1.09202277\n",
      " -0.45168365  0.88321332]\n",
      "OLS estimated beta: [ -4594.58174457  -5794.05369566 -16106.83482789   2537.96474564\n",
      " -20926.22612195   6882.38422856   1947.54105308  -8592.97158243\n",
      "   1881.4021397   14220.50340837   7104.77141908   2674.25812034\n",
      "   -556.8779624   -5135.58455923  25405.97398904 -12923.10730094\n",
      "   7244.48361127 -15465.88839356  23843.98434501   5982.93541493\n",
      "    657.83643116    303.09282762   4328.59634515   -720.03371849\n",
      "  -6852.75144497  -5656.20832979   9576.27308336    755.25890776\n",
      " -11696.10420832   2533.25589156   9624.41909331  11002.24874853\n",
      "  -1752.55711419  -4369.5049089  -10505.11711087  12208.6203449\n",
      "   5706.6515336     620.95407088   2800.01839107    781.33987599\n",
      "  12661.99278062  -1137.71995329 -16021.34548968   -834.93342626\n",
      "   -404.27049909    514.11979727   9724.52766708  16061.11955206\n",
      "    969.98874988   4597.33134751   5796.7067284   16107.60840224\n",
      "  -2538.85103563  20926.72555763  -6885.37956355  -1947.84711155\n",
      "   8596.0629129   -1880.72199485 -14219.07811462  -7108.85598295\n",
      "  -2673.14110701    560.15034711   5138.52794271 -25404.9666643\n",
      "  12922.98673293  -7244.84117473  15463.89439615 -23844.2385175\n",
      "  -5983.91415484   -655.94080304   -304.08486003  -4328.29767032\n",
      "    718.67496481   6852.97697615   5656.38143551  -9576.10194558\n",
      "   -755.06424912  11695.26704979  -2533.95463328  -9625.81607722\n",
      " -11000.41607626   1748.43271093   4370.90626439  10504.74037417\n",
      " -12207.80187654  -5707.76014116   -621.9863826   -2797.41776713\n",
      "   -783.98209074 -12664.42368809   1139.28738176  16023.3942347\n",
      "    836.33525096    402.25886819   -513.81172902  -9726.48408825\n",
      " -16061.69357997   -968.86593289]\n",
      "true beta: [ 2.42641206  0.80906111 -0.08258984  0.26412878  0.24586253  0.54893322\n",
      "  0.5108736   1.9330897  -1.18142018  1.28547069  0.01631126 -0.50454241\n",
      "  0.92453126  1.92920279  1.45856365 -0.13693165  1.40792122  2.07326181\n",
      " -0.08187598 -0.04873566  1.44610236  0.06420453  0.2342469  -0.23625903\n",
      " -1.15442273 -1.09620817  0.55339137  0.13782371 -1.13730378  0.91387401\n",
      " -0.73231903  0.21661657 -2.14330352 -0.41920536 -1.59883385  0.26727102\n",
      "  0.1049904  -0.57121147  1.8752228  -0.40389736 -1.82727288  0.45001689\n",
      " -0.94729734  1.65213702 -0.52063363  0.16892646 -0.43913895 -0.68140722\n",
      "  0.12209202  0.117725    0.30856984 -0.46951676 -0.99811458 -0.5127095\n",
      " -1.82958987 -1.47838755 -0.11544498 -0.24354863 -0.92503349 -2.79329756\n",
      "  0.88376489  1.52287388  0.5459564  -1.13256562 -0.77367859 -0.35586903\n",
      " -1.73396326 -0.41357439 -1.41574803  0.2044152  -0.70396315  0.6662901\n",
      " -1.13447711  0.65534858  0.64281776  0.10153661  0.13418845  0.24682213\n",
      " -1.61944238 -0.1450285   0.40419774 -1.42034562  1.13221307  0.64059342\n",
      "  0.12732098 -0.86364711 -0.8896446   0.42050921 -0.16144523  0.99522617\n",
      "  0.20543822  2.20415765  0.28004546  0.4252783  -1.11467745 -1.8142948\n",
      " -0.42932256  1.79400454]\n"
     ]
    }
   ],
   "source": [
    "# before doing more, let's compare the weights we get from ridge to the OLS weights & true weights\n",
    "print(\"ridge estimated beta:\", beta_estimate_ridge)\n",
    "print(\"OLS estimated beta:\", beta_estimate)\n",
    "print(\"true beta:\", beta_true)\n",
    "\n",
    "# you should see that the size of the ridge estimates is much closer to the original weights, which is good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 104.2307280153736\n",
      "training R^2: 0.988429044063854\n"
     ]
    }
   ],
   "source": [
    "# Now let's compute the training loss\n",
    "training_loss_ridge = ((X_train.dot(beta_estimate_ridge) - Y_train)**2).sum()\n",
    "print(\"training loss:\", training_loss_ridge)\n",
    "print(\"training R^2:\", 1 - training_loss_ridge / (Y_train**2).sum())\n",
    "\n",
    "# the loss value should be bigger than before (about 104 vs. 2.5)\n",
    "# and the training R^2 should be smaller than before (about 0.988 instead of 0.999)\n",
    "# why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between true & predicted Y_test: 0.6025282035249678\n"
     ]
    }
   ],
   "source": [
    "# finally let's test how well our model generalizes to the test dataset\n",
    "Y_test_pred = X_test.dot(beta_estimate_ridge)\n",
    "print(\"Correlation between true & predicted Y_test:\", np.corrcoef(Y_test, Y_test_pred)[0,1])\n",
    "\n",
    "# this should be MUCH HIGHER than before! (0.6 vs. 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression for an fMRI analysis\n",
    "\n",
    "Let's try to use this method now for the same [fMRI experiment](https://www.nature.com/articles/nature17637) that we tried with OLS. The model will be essentially identical to the OLS model, the only difference being that we will use ridge regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load up the feature matrices\n",
    "# these were stored as \"sparse\" matrices in order to save space\n",
    "# but we'll convert them back to normal matrices in order to use them in our regression\n",
    "from scipy import sparse\n",
    "training_features = sparse.load_npz('/Users/clmnlab/Downloads/data/we_word_embeddings/indicator_Rstim.npz').todense().A\n",
    "test_features = sparse.load_npz('/Users/clmnlab/Downloads/data/we_word_embeddings/indicator_Pstim.npz').todense().A\n",
    "\n",
    "# and the brain responses\n",
    "import tables\n",
    "response_tf = tables.open_file('/Users/clmnlab/Downloads/data/we_word_embeddings/small-fmri-responses.hdf5')\n",
    "training_resp = response_tf.root.zRresp.read()\n",
    "test_resp = response_tf.root.zPresp.read()\n",
    "brain_mask = response_tf.root.mask.read()\n",
    "response_tf.close()\n",
    "\n",
    "# as before, to accurately predict BOLD responses we need to account for hemodynamic delays\n",
    "# we'll do that here by creating multiple time-shifted versions of the same stimulus\n",
    "# this is called a finite impulse response or FIR model\n",
    "\n",
    "from util import make_delayed\n",
    "delays = [1,2,3,4]\n",
    "\n",
    "del_training_features = make_delayed(training_features, delays)\n",
    "del_test_features = make_delayed(test_features, delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If we were doing this for real, we would run this part, which uses cross-validation\n",
    "# # to find the best lambda for each voxel.\n",
    "# # But that's time-consuming and annoying, so we're going to skip it here.\n",
    "\n",
    "# from ridge import bootstrap_ridge\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# alphas = np.logspace(0, 2, 10) # Equally log-spaced alphas between 10 and 1000\n",
    "\n",
    "# wt, corr, alphas, bscorrs, valinds = bootstrap_ridge(del_training_features, training_resp, \n",
    "#                                                      del_test_features, test_resp,\n",
    "#                                                      alphas, nboots=1, chunklen=40, nchunks=20,\n",
    "#                                                      singcutoff=1e-10, single_alpha=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fit this ridge model we're going to use some code I wrote instead of the simple equation above\n",
    "# this code is part of a package that does the really hard part of ridge regression,\n",
    "# which is choosing the best lambda (called alpha here, apologies)\n",
    "# here we are skipping that step, and just using a value that I know works pretty well\n",
    "# if you want to see how the more complicated procedure works, \n",
    "# check out the `bootstrap_ridge` function in ridge.py\n",
    "\n",
    "from ridge import ridge\n",
    "beta_ridge = ridge(del_training_features, training_resp, alpha=15.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10808, 5156)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_ridge.shape\n",
    "# as before, should be total number of features (10808) by number of voxels (5156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's test our regression models on the held-out test data\n",
    "pred_test_resp = del_test_features.dot(beta_ridge)\n",
    "\n",
    "import npp # a set of convenience functions I think are missing from numpy :)\n",
    "\n",
    "test_correlations = npp.mcorr(test_resp, pred_test_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 261,
       "width": 400
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the histogram of correlations!\n",
    "plt.hist(test_correlations, 50)\n",
    "plt.xlim(-1, 1)\n",
    "plt.xlabel(\"Linear Correlation\")\n",
    "plt.ylabel(\"Num. Voxels\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better than before! We now have much bigger positive correlations than negative ones. This model kind of works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'build/bdist.macosx-10.9-x86_64/wheel/pycortex-1.2.0.data/data/share/pycortex/db/S1/surfaces'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b09c9510c102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcorr_volume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcortex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVolume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_correlations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'S1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fullhead'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbrain_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RdBu_r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcortex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquickshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_volume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_curvature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/sampark/lib/python3.7/site-packages/cortex/quickflat/view.py\u001b[0m in \u001b[0;36mmake_figure\u001b[0;34m(braindata, recache, pixelwise, thick, sampler, height, dpi, depth, with_rois, with_sulci, with_labels, with_colorbar, with_borders, with_dropout, with_curvature, extra_disp, with_connected_vertices, overlay_file, linewidth, linecolor, roifill, shadow, labelsize, labelcolor, cutout, curvature_brightness, curvature_contrast, curvature_threshold, fig, extra_hatch, colorbar_ticks, colorbar_location, roi_list, nanmean, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# Add data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     data_im, extents = composite.add_data(ax, dataview, pixelwise=pixelwise, thick=thick, sampler=sampler,\n\u001b[0;32m--> 143\u001b[0;31m                                           height=height, depth=depth, recache=recache, nanmean=nanmean)\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_im\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/sampark/lib/python3.7/site-packages/cortex/quickflat/composite.py\u001b[0m in \u001b[0;36madd_data\u001b[0;34m(fig, braindata, height, thick, depth, pixelwise, sampler, recache, nanmean)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;31m# Generate image (2D array, maybe 3D array)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     im, extents = make_flatmap_image(dataview, recache=recache, pixelwise=pixelwise, sampler=sampler,\n\u001b[0;32m--> 163\u001b[0;31m                                      height=height, thick=thick, depth=depth, nanmean=nanmean)\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;31m# Check whether dataview has a cmap instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mcmapdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_has_cmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/sampark/lib/python3.7/site-packages/cortex/quickflat/utils.py\u001b[0m in \u001b[0;36mmake_flatmap_image\u001b[0;34m(braindata, height, recache, nanmean, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \"\"\"\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_flatmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbraindata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbraindata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xfmname\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/sampark/lib/python3.7/site-packages/cortex/quickflat/utils.py\u001b[0m in \u001b[0;36mget_flatmask\u001b[0;34m(subject, height, recache)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcachefile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrecache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_flatmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcachefile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/sampark/lib/python3.7/site-packages/cortex/quickflat/utils.py\u001b[0m in \u001b[0;36m_make_flatmask\u001b[0;34m(subject, height)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpolyutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0mpts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_surf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"flat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnudge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m     \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolyutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolyutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboundary_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/sampark/lib/python3.7/site-packages/cortex/database.py\u001b[0m in \u001b[0;36mmemofn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_memocache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_memocache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_memocache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/sampark/lib/python3.7/site-packages/cortex/database.py\u001b[0m in \u001b[0;36mget_surf\u001b[0;34m(self, subject, type, hemisphere, merge, nudge)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'surfs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhemisphere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"both\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/sampark/lib/python3.7/site-packages/cortex/database.py\u001b[0m in \u001b[0;36mget_paths\u001b[0;34m(self, subject)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0msurfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msurf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m             \u001b[0mssurf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssurf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'build/bdist.macosx-10.9-x86_64/wheel/pycortex-1.2.0.data/data/share/pycortex/db/S1/surfaces'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 322,
       "width": 477
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's also look at a brain map of the correlations!\n",
    "\n",
    "import cortex\n",
    "\n",
    "corr_volume = cortex.Volume(test_correlations, 'S1', 'fullhead', mask=brain_mask, vmin=-0.3, vmax=0.3, cmap='RdBu_r')\n",
    "cortex.quickshow(corr_volume, with_curvature=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also look at it in 3D!\n",
    "\n",
    "cortex.webshow(corr_volume, open_browser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting ridge regression as a \"Gaussian prior\"\n",
    "\n",
    "So we've introduced ridge regression as a \"penalty\" on the loss function, but there's another way to think about ridge regression that is pretty useful in some situations. I want to note that this second interpretation is _exactly mathematically equivalent_ to what you've already seen, it's just a different way of thinking about it.\n",
    "\n",
    "Remember Bayes' rule? $P(a|b) = \\frac{P(b|a) P(a)}{P(b)}$, right? It turns out we can use Bayes' rule to derive ridge regression if we plug in the right variables in the right places. To do this we kind of have to go back to the beginning, but I think it's a much quicker trip once you've already gotten to this point once.\n",
    "\n",
    "From a Bayesian viewpoint, the goal of regression is to find the most probably set of weights ($\\beta$) given the observed data ($x(t), y(t)$). Mathematically, that means we're trying to find the set of weights $\\beta$ that maximize $P(\\beta | x(t), y(t))$ (read as \"probability of $\\beta$ given $x(t)$ and $y(t)$\").\n",
    "\n",
    "We don't really know what $P(\\beta | x(t),y(t))$ is—that's not how we defined our regression model—but we DO know what $P(y(t) | x(t), \\beta)$ is! The assumptions that we've been working with so far (squared error loss, etc.), imply that $P(y(t) | x(t), \\beta)$ is actually a Gaussian distribution (aka Normal distribution). The mean of this distribution is (we assume) $x(t) \\beta$, and it has some variance that we don't know ahead of time (this is the size of the noise term $\\epsilon(t)$). We can write that like this:\n",
    "\n",
    "$$ P(y(t) | x(t), \\beta) = \\mathcal{N}(x(t) \\beta, \\sigma^2), $$\n",
    "where:\n",
    "* $\\mathcal{N}(x(t) \\beta, \\sigma^2)$ denotes a Normal distribution with mean $x(t) \\beta$ and variance $\\sigma^2$.\n",
    "* $\\sigma^2$ is the size of the noise. We don't know what this is yet!\n",
    "\n",
    "Ok now we kind of know what $P(y(t) | x(t), \\beta)$ is, and we know Bayes' rule. We can combine these things to figure out $P(\\beta | y(t), x(t))$, which is the thing we actually want to maximize! Let's see how that goes..\n",
    "\n",
    "$$ P(\\beta | y(t), x(t)) = \\frac{P(y(t) | x(t), \\beta) P(\\beta)}{P(y(t))} $$\n",
    "\n",
    "Huh ok. Bayes' rule says that in order to get the thing we want ($P(\\beta | y(t), x(t))$), we take the thing we know ($P(y(t) | x(t), \\beta)$), multiply it by some weird new thing ($P(\\beta)$), and then divide it by some other weird new thing ($P(y(t)$). Let's step through these different terms, what they mean, and how we can handle them:\n",
    "* $P(y(t) | x(t), \\beta)$. This is the thing we already know. It's sometimes called the \"likelihood\" function. We're modeling it as a Gaussian distribution. All good!\n",
    "* $P(\\beta)$. This is a new one. It's called the \"prior\" distribution on $\\beta$. Think of it as \"given absolutely no information about any of the data, what values of $\\beta$ do we think are more or less likely?\" (Foreshadowing: _this is where the magic will happen._)\n",
    "* $P(y(t))$. This is also a new one. Fortunately, we can ignore it entirely! It doesn't matter one bit what the value of this function is, because it's independent of $\\beta$, so our choice of the best $\\beta$ is unaffected by it! Whew.\n",
    "\n",
    "So clearly the most important new thing here is the prior, $P(\\beta)$. **Let's suppose that $P(\\beta)$ is another Gaussian distribution.** That is, let's say that $P(\\beta) = \\mathcal{N}(0, \\lambda^{-1} I)$, a multivariate Gaussian distribution (because $\\beta$ typically is a vector containing more than one value) with mean zero and covariance $\\lambda^{-1} I$. You may have an inkling of where this is heading now..\n",
    "\n",
    "Let's go back to our full expression for $P(\\beta | y(t), x(t))$ (which is called the posterior distribution on $\\beta$), plug some things in, and see what happens!\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "P(\\beta | y(t), x(t)) & = & \\frac{P(y(t) | x(t), \\beta) P(\\beta)}{P(y(t))} \\\\\n",
    "P(\\beta | y(t), x(t)) & \\propto & P(y(t) | x(t), \\beta) P(\\beta) \\;\\mbox{ (dropping the denominator)}\\\\\n",
    "P(\\beta | y(t), x(t)) & \\propto & e^{-\\frac{1}{2 \\sigma^2} (y(t) - x(t)\\beta)^2} e^{-\\frac{1}{2} \\beta^\\top (\\lambda^{-1} I)^{-1} \\beta} \\; \\mbox{ (plugging in the definition of a Gaussian distribution)} \\\\\n",
    "P(\\beta | y(t), x(t)) & \\propto & e^{-\\frac{1}{2 \\sigma^2} (y(t) - x(t)\\beta)^2 -\\frac{1}{2} \\beta^\\top (\\lambda^{-1} I)^{-1} \\beta} \\;\\mbox{ (combining exponentials)}\\\\\n",
    "P(\\beta | y(t), x(t)) & \\propto & e^{-\\frac{1}{2 \\sigma^2} (y(t) - x(t)\\beta)^2 -\\frac{1}{2} \\lambda \\beta^\\top \\beta}\\\\\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Alright that was a wild ride, but we ended up with something pretty interesting. The most important thing here is to look at the exponent in our new expression.. does it look familiar? It should, because it's pretty much the same as the ridge loss function (or, at least, the negative of the ridge loss function)! Recall that our goal here is to maximize the probability of $\\beta$ given $y(t)$ and $x(t)$. This equation shows us that maximizing that probability is _exactly equivalent_ to minimizing the ridge loss function. The prior $P(\\beta)$ and the penalty term in the loss function $\\lambda \\beta^\\top \\beta$ _mean exactly the same thing_!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sampark",
   "language": "python",
   "name": "sampark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
