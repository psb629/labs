{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Encoding Models with Word Embeddings\n",
    "originally for NeuroHackademy 2020, by Alex Huth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some basic stuff we'll need later\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Tikhonov Regression\n",
    "\n",
    "## How does changing the features before regression affect the result?\n",
    "\n",
    "Now that you've learned about what ridge regression is, and (more or less) how it works, we can play around with it a bit and see what neat things we can make happen.\n",
    "\n",
    "Let's start with something simple, working from an example with toy data. Let's modify our $x(t)$ by dividing the value of the first feature by 10. (Why? Don't worry about that yet. Just see where this goes.) This means we are replacing our original $x(t)$ by something new, we'll call it $x'(t)$, and define it like this (remember that $x(t)$ is a vector that contains $p$ different features):\n",
    "\n",
    "$$x'(t) = \\begin{bmatrix} \\frac{x_1(t)}{10} & x_2(t) & \\dots & x_p(t) \\end{bmatrix} $$\n",
    "\n",
    "What would the result of this change be for OLS regression?\n",
    "\n",
    "More specifically: if we did OLS regression with the responses $y(t)$ and modified stimulus features $x'(t)$, we'd obtain a new set of weights $\\beta'_{OLS}$. What do you think the relationship is between $\\beta'_{OLS}$ and $\\beta_{OLS}$, the weights we would've gotten with the original $x(t)$? Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we did earlier, let's create some fake data so we can test things out\n",
    "T_train = 100\n",
    "T_test = 25\n",
    "p = 5\n",
    "noise_size = 10.0 # the standard deviation of the noise, epsilon\n",
    "\n",
    "X_train = np.random.randn(T_train, p)\n",
    "X_test = np.random.randn(T_test, p)\n",
    "\n",
    "beta_true = np.random.randn(p)\n",
    "\n",
    "Y_train = X_train.dot(beta_true) + noise_size * np.random.randn(T_train)\n",
    "Y_test = X_test.dot(beta_true) + noise_size * np.random.randn(T_test)\n",
    "\n",
    "# And let's estimate the weights using the original features, X_train\n",
    "beta_estimate_orig = np.linalg.lstsq(X_train, Y_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create our modified X\n",
    "X_train_mod = X_train.copy()\n",
    "X_train_mod[:,0] /= 10.0 # divide the first feature by 10\n",
    "\n",
    "# And re-estimate the weights using this one\n",
    "beta_estimate_mod = np.linalg.lstsq(X_train_mod, Y_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's compare the estimated weights!\n",
    "print(\"orig beta:\", beta_estimate_orig)\n",
    "print(\"mod  beta:\", beta_estimate_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! What you should've found (spoiler alert) is that, when you're using OLS, making one of the features smaller by a factor of 10 just makes the corresponding weight value _bigger_ by a factor of 10. This makes the predictions of this new model _exactly the same_ as the predictions of the old model:\n",
    "\n",
    "$$ x'(t)\\beta' = x(t) \\beta $$\n",
    "\n",
    "Which.. makes sense, right? If you're finding the $\\beta$ (well, $\\beta'$, in this case) that minimizes the error perfectly, then your regression method shouldn't really care about silly little things like multiplying one of your features by 10.\n",
    "\n",
    "So what about ridge? If we did ridge regression with $y(t)$ and $x'(t)$, obtaining new weights $\\beta'_{ridge}$, how would those weights be related to the weights $\\beta_{ridge}$ that you'd get from using the original $x(t)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ridge import ridge\n",
    "\n",
    "beta_est_ridge_orig = ridge(X_train, Y_train[:,None], alpha=1.0)\n",
    "beta_est_ridge_mod = ridge(X_train_mod, Y_train[:,None], alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's compare the estimated weights!\n",
    "print(\"ridge orig beta:\", beta_est_ridge_orig.ravel())\n",
    "print(\"ridge mod  beta:\", beta_est_ridge_mod.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright the result here is _really different_. Not only does the weight on the first feature not increase by a factor of 10, the weights on the other features have changed as well. Unlike the OLS case, this model is _not_ equivalent to the original ridge model! What's going on here?\n",
    "\n",
    "When we tested OLS, the regression procedure was able to correct for our modification by changing the weights. In particular, it made the weight on the feature that we modified 10x bigger. But in ridge regression, it's _costly_ to make the weights big. Remember that we penalize the loss by a factor of $\\beta^2$. So in order to make the weight 10x bigger, the penalty (at least for that one parameter) needs to increase by _100x_.\n",
    "\n",
    "The result is that the weight is _not_ simply increased by 10x, it's only increased by about 5x. But there's more than that going on! The _other_ weights have also changed. Why did that happen? Setting the first weight to a smaller value than it should have been creates _new errors_ in the prediction of $y(t)$. To account for these errors, the model will change the values of the other weights in $\\beta$ as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _WHAT HAVE WE DONE?_\n",
    "\n",
    "We just did an extremely simple thing: divided one of our feature values by 10. And it changed our entire model! How does this fit into any of the mathematical formalisms that we were dealing with earlier?\n",
    "\n",
    "Let's redefine what we've done here more formally. This will help us discover what it is that we've managed to acomplish with this little stunt.\n",
    "\n",
    "The only thing we did was divide one of the features by 10. Let's represent that as a matrix multiplication: $X A$. Remember that $X$ is a $T \\times p$ matrix ($T$ rows, one for each \"timepoint\" in our dataset, and $p$ columns, one for each feature). Let's define $A$ as a $p \\times p$ matrix that looks like this:\n",
    "\n",
    "$$ A = \\begin{bmatrix} \n",
    "0.1 & 0 & 0 & \\dots \\\\ \n",
    "0 & 1 & 0 & \\dots \\\\\n",
    "0 & 0 & 1 & \\dots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$A$ is a diagonal matrix (meaning it only has non-zero values on the main diagonal), and all of the values on the diagonal are 1 except the first, which we have set to 0.1 (i.e. dividing by 10). Multiplying this matrix on the right side of $X$ will do exactly what we did by hand before: scale the first feature down by a factor of 10.\n",
    "\n",
    "So we can write the new model that we're trying to fit like this:\n",
    "\n",
    "$$ Y = X A \\beta + \\epsilon $$\n",
    "\n",
    "And we know that when we did this using OLS, we found that the first weight had _increased_ by a factor of 10. That's like multiplying $\\beta$ by the inverse of $A$, giving you:\n",
    "\n",
    "$$ \\beta'_{OLS} = A^{-1} \\beta_{OLS} $$\n",
    "\n",
    "If we try to combine these two things and make predictions using this new scaled model, we see that everything cancels out nicely:\n",
    "\n",
    "$$ X A \\beta'_{OLS} = X A A^{-1} \\beta_{OLS} = X \\beta_{OLS} $$\n",
    "\n",
    "Why did this work out so nicely? It's because the OLS equation for $\\beta$:\n",
    "\n",
    "$$ \\beta_{OLS} = (X^\\top X)^{-1} X^\\top Y $$\n",
    "becomes\n",
    "$$ \\beta'_{OLS} = (A^\\top X^\\top X A)^{-1} A^\\top X^\\top Y $$\n",
    "where we can [pop the $A$ and $A^\\top$ out of the inverse](https://en.wikipedia.org/wiki/Invertible_matrix#Other_properties), giving:\n",
    "$$ \\begin{eqnarray}\n",
    "\\beta'_{OLS} &=& A^{-1}(X^\\top X)^{-1} (A^\\top)^{-1} A^\\top X^\\top Y \\\\\n",
    "&=& A^{-1}(X^\\top X)^{-1} X^\\top Y \\\\\n",
    "&=& A^{-1}\\beta_{OLS}\\\\\n",
    "\\end{eqnarray} $$\n",
    "confirming what we found empirically! Nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _WHAT HAVE WE DONE? (BUT FOR RIDGE)_\n",
    "\n",
    "So what happened in the ridge case? Let's try to do the same trick with our ridge equation, $\\beta_{ridge} = (X^\\top X + \\lambda I)^{-1} X^\\top Y$:\n",
    "\n",
    "$$ \\beta'_{ridge} = (A^\\top X^\\top X A + \\lambda I)^{-1} A^\\top X^\\top Y $$\n",
    "We can't just pop the $A$ and $A^\\top$ out of the inverse like we did before because there's a sum inside there.\n",
    "\n",
    "But we can pop the outer $A^\\top$ _into_ the inverse from the right side (its inverse ends up on the left side of each term), giving:\n",
    "\n",
    "$$ \\begin{eqnarray} \\beta'_{ridge} &=& ((A^\\top)^{-1} A^\\top X^\\top X A + \\lambda (A^\\top)^{-1})^{-1} X^\\top Y \\\\\n",
    "&=& (X^\\top X A + \\lambda (A^\\top)^{-1})^{-1} X^\\top Y \\end{eqnarray}$$\n",
    "\n",
    "Then we can use the old trick of multiplying by 1 (well, $I$) in the form of $A^{-1} A$, then pop the $A$ inside the inverse, giving:\n",
    "\n",
    "$$\\begin{eqnarray} \\beta'_{ridge} &=& (A^{-1} A) (X^\\top X A + \\lambda (A^\\top)^{-1})^{-1} X^\\top Y \\\\\n",
    "&=& A^{-1} (X^\\top X A A^{-1} + \\lambda (A^\\top)^{-1} A^{-1})^{-1} X^\\top Y \\\\\n",
    "&=& A^{-1} (X^\\top X + \\lambda (A A^\\top)^{-1})^{-1} X^\\top Y \n",
    "\\end{eqnarray} $$\n",
    "\n",
    "Here we can see why the ridge solution came out just _different_. Similar to the OLS solution, it has this leading factor of $A^{-1}$, which is effectively multiplying the first weight by 10 here. But unlike the OLS solution, we still have this weird stuff sitting inside our big matrix inverse: the penalty factor is now $\\lambda (A A^\\top)^{-1}$ instead of just $\\lambda I$. This is why all the other weights changed and not just the weight on the first feature.\n",
    "\n",
    "To understand what this $(A A^\\top)^{-1}$ factor is doing, let's go back to our Bayesian formulation, and in particular the prior on $\\beta$. A [multivariate Gaussian distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) on $\\beta$ with mean zero and covariance $\\Sigma$ has the following form (I'm dropping the constant in front for convenience):\n",
    "$$ P(\\beta) \\propto e^{-\\frac{1}{2} \\beta^{\\top} \\Sigma^{-1} \\beta} $$\n",
    "\n",
    "Originally we had set $\\Sigma = \\lambda^{-1} I$, making $\\Sigma^{-1} = \\lambda I$, which was the original factor in the ridge regression equation. Here, instead of $\\lambda I$, we have $\\lambda (A A^\\top)^{-1}$. This suggests that we can interpret what we've done here—dividing the first feature in $X$ by 10—as _choosing a different prior for $\\beta$_.\n",
    "\n",
    "Specifically, we seem to have chosen $\\Sigma = \\lambda^{-1} A A^\\top$. This prior says that each of the weights in $\\beta$ has a prior variance of $\\lambda^{-1}$ except the first, which has a prior variance of $\\lambda^{-1} / 100$. Thus, the model is different because we pretty much told it that we expect the first weight to be much smaller than the others!\n",
    "\n",
    "What we've rediscovered here, through this very simple manipulation, is an advanced regression technique called [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization#Tikhonov_regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implications for data preprocessing before regularized regression\n",
    "\n",
    "One of the key things you might want to take away from this is that inconsequential-seeming things, like scaling features appropriately, can have huge effects on regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formal definition of Tikhonov regression\n",
    "\n",
    "When we introduced the Bayesian interpretation of ridge regression, we created a prior distribution on $\\beta$ that, more or less, suggested to the regression model that the weights should be small. We defined this prior as a multivariate Gaussian distribution with mean zero and covariance matrix $\\lambda^{-1} I$, i.e. equal variance (of $\\lambda^{-1}$) on each weight with zero covariance between weights.\n",
    "\n",
    "In Tikhonov regression we are simply generalizing this idea. Instead of assuming that the prior covariance is a scaled identity matrix, we can assume _any_ covariance matrix we want!\n",
    "\n",
    "And what's more, as we've already seen, we can do Tikhonov regression using the standard ridge regression tools that are already available to us. Let's run through how to do that:\n",
    "\n",
    "### Tikhonov regression via ridge regression\n",
    "\n",
    "1. Suppose we know our responses $Y$ and stimulus features $X$. We want to fit a Tikhonov regression model of the form $Y = X\\beta + \\epsilon$, with the prior $P(\\beta) = \\mathcal{N}(0, \\lambda^{-1} \\Sigma)$, for some covariance $\\Sigma$.\n",
    "2. We use some technique (there are many) to take a matrix square root of $\\Sigma$, giving $\\sqrt{\\Sigma} = A^\\top$, so that $A A^\\top = \\Sigma$.\n",
    "3. We are going to use this new matrix to transform our stimulus features, and then fit a ridge regression model for $Y = (XA)\\beta' + \\epsilon$.\n",
    "4. The resulting ridge weights are $\\beta' = A^{-1} (X^\\top X + \\lambda (A A^\\top)^{-1})^{-1} X^\\top Y$.\n",
    "5. Finally, we multiply these weights by $A$ to get the Tikhonov weights (this corrects for the factor of $A^{-1}$, giving us weights that can be applied to the original $X$): $\\beta = A \\beta'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings for Tikhonov regression models\n",
    "\n",
    "Now let's get back to our fMRI experiment. We had subjects listen to stories and then we tried to predict the response of each voxel using a regression model where each word was a feature.\n",
    "\n",
    "It turns out this didn't work terribly well using our OLS or ridge models. We also had this problem where some words might appear in our test set but not the training set, so we couldn't estimate weights for them at all.\n",
    "\n",
    "Let's try to fix this model using Tikhonov regression! This will require one new concept: a **word embedding space**.\n",
    "\n",
    "## Word embeddings\n",
    "\n",
    "[Word embedding spaces](https://en.wikipedia.org/wiki/Word_embedding) are a tool for quantitatively modeling something related to the meaning of words. Famous examples of word embedding spaces are [word2vec](https://en.wikipedia.org/wiki/Word2vec) and [GloVe](https://en.wikipedia.org/wiki/GloVe_(machine_learning%29). The core idea of word embeddings is that they represent each word as a vector of numbers, where these vectors are specifically selected so that words with similar (or related) meanings have similar vectors.\n",
    "\n",
    "For this exercise we're going to use a word embedding space of my own design called `english1000`. Let's load that space here and play with it a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load semantic model\n",
    "# The SemanticModel class is something I wrote to make it easy to deal with word embedding spaces\n",
    "from SemanticModel import SemanticModel\n",
    "eng1000 = SemanticModel.load(\"data/we_word_embeddings/small_english1000sm.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a word\n",
    "First let's plot the length 985 vector for one word to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word = \"finger\"\n",
    "\n",
    "f = plt.figure(figsize=(15,5))\n",
    "ax = f.add_subplot(1,1,1)\n",
    "ax.plot(eng1000[plot_word], 'k')\n",
    "ax.axis(\"tight\")\n",
    "ax.set_title(\"English1000 representation for %s\" % plot_word)\n",
    "ax.set_xlabel(\"Feature number\")\n",
    "ax.set_ylabel(\"Feature value\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing more than one word\n",
    "Next let's plot the vectors for three words: \"finger\", \"hand\", and \"language\". Here you will see that \"finger\" (in black) and \"hand\" (in red) look pretty similar, but \"language\" (in blue) looks very different. Neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_words = [\"finger\", \"hand\", \"language\"]\n",
    "colors = [\"k\", \"r\", \"b\"]\n",
    "\n",
    "f = plt.figure(figsize=(15,5))\n",
    "ax = f.add_subplot(1,1,1)\n",
    "wordlines = []\n",
    "\n",
    "for ii, (word, color) in enumerate(zip(plot_words, colors)):\n",
    "    wordlines.append(ax.plot(eng1000[word] - 8*ii, color)[0])\n",
    "\n",
    "ax.axis(\"tight\")\n",
    "ax.set_title(\"English1000 representations for some words\")\n",
    "ax.set_xlabel(\"Feature number\")\n",
    "ax.legend(wordlines, plot_words);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic smoothness\n",
    "One nice test of a vector-space semantic model is whether it results in a \"semantically smooth\" representation of the words. That is, do nearby words in the space have intuitively similar meanings? Here you can test that using the method `find_words_like_word`. \n",
    "\n",
    "Give any word (that the model knows about), and it will print out the 10 closest words (that it knows about) and their cosine similarities (or correlations, same thing in this case). This includes the word you supplied.\n",
    "\n",
    "You can put different words in here and see what the model comes up with. \n",
    "\n",
    "*(Be warned: the model knows some dirty words. It was trained using the internet, after all.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic model\n",
    "eng1000.find_words_like_word(\"finger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is just another example, but this one an abstract noun, \"language\". Again the model does a pretty good job at finding related words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng1000.find_words_like_word(\"language\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little more generally, we can grab the vectors for a set of words and then look at how related each pair of vectors is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from covplot import covplot\n",
    "\n",
    "sel_words = ['woman', 'girl', 'boy', 'man', 'street', 'park', 'alley', 'house']\n",
    "sel_word_vectors = np.vstack([eng1000[w] for w in sel_words])\n",
    "print(sel_word_vectors.shape)\n",
    "\n",
    "sel_word_products = sel_word_vectors.dot(sel_word_vectors.T) / sel_word_vectors.shape[1]\n",
    "covplot(sel_word_products)\n",
    "\n",
    "plt.gca().xaxis.tick_top()\n",
    "plt.xticks(range(len(sel_words)), sel_words, fontsize=15, rotation=90)\n",
    "plt.yticks(range(len(sel_words)), sel_words, fontsize=15)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a word embedding space for Tikhonov regression\n",
    "\n",
    "We're going to use these word embeddings to do Tikhonov regression for our fMRI experiment. Let's call the (number of embedding features $\\times$ number of words) matrix of word embeddings $E$. We're going to choose the prior covariance for our regression weights to be proportional to $E^\\top E$, i.e.\n",
    "\n",
    "$$P(\\beta) = \\mathcal{N}(0, \\lambda^{-1} E^\\top E) $$\n",
    "\n",
    "__This means that we expect (a priori) the regression weights on two words to be similar if those words have similar embedding vectors.__\n",
    "\n",
    "For example, the words \"woman\" and \"man\" have very similar embedding vectors, according to the plot we created above. If we use the embedding vectors to create our Tikhonov prior, then we would be suggesting to our model that, if a voxel responds a lot to the word \"woman\", it probably also responds a lot to the word \"man\", and vice versa.\n",
    "\n",
    "So how do we do this? We can partially follow the recipe from above, but we're actually going to have an easier time here than we would in the generic case since we don't need to take a matrix square root. We've already defined our prior covariance as $E^\\top E$, so all we have to do is say $A = E^\\top$. Let's give it a shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, let's load up the feature matrices\n",
    "# these were stored as \"sparse\" matrices in order to save space\n",
    "# but we'll convert them back to normal matrices in order to use them in our regression\n",
    "from scipy import sparse\n",
    "training_features = sparse.load_npz('data/we_word_embeddings/indicator_Rstim.npz').todense().A\n",
    "test_features = sparse.load_npz('data/we_word_embeddings/indicator_Pstim.npz').todense().A\n",
    "\n",
    "# and the brain responses\n",
    "import tables\n",
    "response_tf = tables.open_file('data/we_word_embeddings/small-fmri-responses.hdf5')\n",
    "training_resp = response_tf.root.zRresp.read()\n",
    "test_resp = response_tf.root.zPresp.read()\n",
    "brain_mask = response_tf.root.mask.read()\n",
    "response_tf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll apply the word embedding, multiplying it by both the training and test feature matrices\n",
    "\n",
    "emb_training_features = training_features.dot(eng1000.data.T)\n",
    "emb_test_features = test_features.dot(eng1000.data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as before, to accurately predict BOLD responses we need to account for hemodynamic delays\n",
    "# we'll do that here by creating multiple time-shifted versions of the same stimulus\n",
    "# this is called a finite impulse response or FIR model\n",
    "\n",
    "from util import make_delayed\n",
    "delays = [1,2,3,4]\n",
    "\n",
    "del_training_features = make_delayed(emb_training_features, delays)\n",
    "del_test_features = make_delayed(emb_test_features, delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fit this ridge model we're going to use some code I wrote instead of the simple equation above\n",
    "# this code is part of a package that does the really hard part of ridge regression,\n",
    "# which is choosing the best lambda (called alpha here, apologies)\n",
    "# here we are skipping that step, and just using a value that I know works pretty well\n",
    "# if you want to see how the more complicated procedure works, \n",
    "# check out the `bootstrap_ridge` function in ridge.py\n",
    "\n",
    "from ridge import ridge\n",
    "beta_tik = ridge(del_training_features, training_resp, alpha=464.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's test our regression models on the held-out test data\n",
    "pred_test_resp = del_test_features.dot(beta_tik)\n",
    "\n",
    "import npp # a set of convenience functions I think are missing from numpy :)\n",
    "\n",
    "test_correlations = npp.mcorr(test_resp, pred_test_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the histogram of correlations!\n",
    "plt.hist(test_correlations, 50)\n",
    "plt.xlim(-1, 1)\n",
    "plt.xlabel(\"Linear Correlation\")\n",
    "plt.ylabel(\"Num. Voxels\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now _that's_ a lot better! In fact, let's compare it to the OLS and ridge models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_correlations = np.load('data/we_word_embeddings/ols_correlations.npy')\n",
    "ridge_correlations = np.load('data/we_word_embeddings/ridge_correlations.npy')\n",
    "\n",
    "plt.hist(ols_correlations, 50, label='OLS', histtype='step', lw=2)\n",
    "plt.hist(ridge_correlations, 50, label='Ridge', histtype='step', lw=2)\n",
    "plt.hist(test_correlations, 50, label='Tikhonov', histtype='step', lw=2)\n",
    "plt.xlim(-1, 1)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Linear Correlation\")\n",
    "plt.ylabel(\"Num. Voxels\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also look at a brain map of the correlations!\n",
    "\n",
    "import cortex\n",
    "\n",
    "corr_volume = cortex.Volume(test_correlations, 'S1', 'fullhead', mask=brain_mask, vmin=-0.3, vmax=0.3, cmap='RdBu_r')\n",
    "cortex.quickshow(corr_volume, with_curvature=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also look at it in 3D!\n",
    "\n",
    "cortex.webshow(corr_volume, open_browser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We've gone through three different types of regression models: ordinary least squares (OLS), ridge, and Tikhonov (with a prior based on word embeddings).\n",
    "\n",
    "All three models used _exactly the same features_. They only differed in their prior beliefs about the weights:\n",
    "* the OLS model made no assumptions, seeing all possible weights as equally reasonable,\n",
    "* the ridge model assumed that the weights were _small_, i.e. close to zero, and\n",
    "* the Tikhonov model assumed that the weights had a specific structure that matched the word embeddings we used.\n",
    "\n",
    "These different prior beliefs led to _huge_ differences in prediction performance between models. We can think of the different priors as different **hypotheses** about how words might influence BOLD responses in cortex. Comparing the prediction performance of these models is then _testing_ these hypotheses. Our results suggest that the third hypothesis—that voxels respond similarly to words with similar meanings—is by far the most likely, given this brain data.\n",
    "\n",
    "If you want to learn more about the science we can do with these models, again, please check out these papers:\n",
    "* Huth, de Heer, Griffiths, Theunissen, & Gallant (2016) \"Natural speech reveals the semantic maps that tile human cerebral cortex\" [(Free Link)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4852309/)\n",
    "* Nunez-Elizalde, Huth, & Gallant (2019) \"Voxelwise encoding models with non-spherical multivariate normal priors\" [(Journal Link)](https://www.sciencedirect.com/science/article/pii/S1053811919302988) [(Preprint Link)](https://www.biorxiv.org/content/biorxiv/early/2018/08/09/386318.full.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
