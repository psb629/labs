{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "527a5637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, exists\n",
    "from os import makedirs\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, colors\n",
    "from matplotlib.ticker import MultipleLocator, IndexLocator, FuncFormatter\n",
    "\n",
    "from nilearn import image, plotting, masking\n",
    "\n",
    "from joblib import Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd75647",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae6d41c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "745adf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = '/mnt/ext5/DRN'\n",
    "\n",
    "dir_behav = join(dir_root, 'behav_data')\n",
    "\n",
    "dir_fmri = join(dir_root,'fmri_data')\n",
    "dir_mask = join(dir_fmri, 'masks')\n",
    "dir_model = join(dir_root,'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c3922",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c4e15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc803f3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "138b6aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_processing(obs):\n",
    "    vis_obs = []\n",
    "\n",
    "    for _obs in obs:\n",
    "        ## _obs.shape = (1, 72, 128, 3)\n",
    "        vis_obs.append(_obs)\n",
    "\n",
    "    ## visual observation [(1, 72, 128, 3) x 4]\n",
    "    vis_obs = np.concatenate(vis_obs, axis=-1)\n",
    "    ## visual observation (1, 72, 128, 12)\n",
    "    vis_obs = np.transpose(vis_obs, (0, 3, 1, 2))\n",
    "    vis_obs = (vis_obs * 255).astype(np.uint8)\n",
    "    \n",
    "    ## visual observation (1, 12, 72, 128)\n",
    "    return vis_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd69a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_to_sec(Time):\n",
    "    m, s, ds = np.array(Time.split('-')).astype(int)\n",
    "    return m*60+s+0.001*ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac48db",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "367fba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## time points per a run\n",
    "TR = 0.5\n",
    "TPs = 1400 - 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2e8aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_shift = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba352ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 'DRN09'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3814f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc96fd5",
   "metadata": {},
   "source": [
    "### behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce8e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_behav(subj, resolution=(128,72)):\n",
    "    \n",
    "    dir_work = join(dir_behav,subj,'resized_%dx%d'%(resolution[0],resolution[1]))\n",
    "    list_run = ['Run%d'%(ii+1) for ii in range(len(glob(join(dir_work,'*'))))]\n",
    "    \n",
    "    ## i) Screen Shots\n",
    "    list_png = {}\n",
    "    ## ii) Actions\n",
    "    behav = {}\n",
    "    ## iii) onset times\n",
    "    list_onsettime = {}\n",
    "    ## iv) episode\n",
    "    list_episode = {}\n",
    "    for run in list_run:\n",
    "        ## i)\n",
    "        list_png[run] = np.array(sorted(glob(join(dir_work,run,'*.png'))))\n",
    "        ## ii)\n",
    "        with open(join(dir_work,run,'log.json'),'r') as f:\n",
    "            behav[run] = json.load(f)\n",
    "#         print('%s: actions (%d) / pngs (%d)'%(run, len(behav[run]), len(list_png[run])))\n",
    "\n",
    "        ## iii)\n",
    "        tmp = []\n",
    "        ### iv)\n",
    "        tmp2 = []\n",
    "        for dict_ in behav[run]:\n",
    "            tmp.append(convert_time_to_sec(dict_['Time']))\n",
    "            tmp2.append(dict_['Episode'])\n",
    "        list_onsettime[run] = np.array(tmp)\n",
    "        list_episode[run] = np.array(tmp2)\n",
    "    \n",
    "    return list_png, list_onsettime, list_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d1fd92",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1abc98e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_idx_input(subj, run, shift=0):\n",
    "    nFrameStack = 4\n",
    "    TR = 0.5\n",
    "    \n",
    "    _, list_onsettime, _ = get_data_behav(subj)\n",
    "\n",
    "    onsettimes = list_onsettime[run]\n",
    "    onsettimes = onsettimes[onsettimes<700-shift]\n",
    "\n",
    "    ## A set of the last indices for each episode except the last episode\n",
    "    idx_final = np.concatenate([np.where(np.diff(onsettimes)>TR)[0], [onsettimes.shape[0]-1]])\n",
    "    idx_initial = np.concatenate([[0],idx_final[:-1]+1])\n",
    "    assert idx_initial.shape == idx_final.shape\n",
    "\n",
    "    idx_input = {}\n",
    "    idx_epi = {}\n",
    "    for ii, (idx_i, idx_f) in enumerate(zip(idx_initial, idx_final)):\n",
    "        episode = 'episode:%02d'%(ii+1)\n",
    "\n",
    "        ## the times of the initial and final frame\n",
    "        ta, tb = onsettimes[idx_i], onsettimes[idx_f]\n",
    "        ## the number of frames at the episode\n",
    "        n_frame = int(idx_f-idx_i+1)\n",
    "\n",
    "        ## The fMRI image corresponding to the first frame of this episode\n",
    "        a = ta - ta%TR\n",
    "        ## The fMRI image corresponding to the final frame of this episode\n",
    "        b = tb - tb%TR\n",
    "\n",
    "        ## The number of fMRI images representing the corresponding episode.\n",
    "        n_epi = int((b-a)/TR + 1)\n",
    "        if n_epi <= 1:\n",
    "            continue\n",
    "\n",
    "        ## The fMRI indices that make up each episode\n",
    "        timepoint_epi = np.arange(a,b+TR,TR)+shift\n",
    "        timepoint_epi = timepoint_epi[timepoint_epi<700]\n",
    "        idx_epi[episode] = (2*(timepoint_epi-6)).astype(int)\n",
    "\n",
    "        ## an input set to forward via a model\n",
    "        idx_input[episode] = np.zeros((n_epi,nFrameStack)).astype(int)\n",
    "\n",
    "        ## the last input\n",
    "        idx_input[episode][-1] = [idx_f-nFrameStack+1+jj for jj in range(nFrameStack)]\n",
    "\n",
    "        ## the rest of them\n",
    "        didx = (idx_f-nFrameStack-idx_i)*TR/(b-a+1.e-8)\n",
    "        for jj in range(n_epi-1):\n",
    "            idx_input[episode][jj] = [int(idx_i+didx*jj+kk) for kk in range(nFrameStack)]\n",
    "\n",
    "    for episode, input_ in idx_input.items():\n",
    "        assert input_.shape[0] == idx_epi[episode].shape[0]\n",
    "    \n",
    "    return idx_input, idx_epi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b77ab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86233ac7",
   "metadata": {},
   "source": [
    "the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "206f94d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env': {'name': 'drone_hanyang_mlagent', 'render': 'True', 'time_scale': 3.0},\n",
       " 'agent': {'name': 'ppo',\n",
       "  'network': 'continuous_policy_value',\n",
       "  'head': 'cnn',\n",
       "  'gamma': 0.95,\n",
       "  'batch_size': 32,\n",
       "  'n_step': 128,\n",
       "  'n_epoch': 3,\n",
       "  '_lambda': 0.95,\n",
       "  'epsilon_clip': 0.1,\n",
       "  'vf_coef': 1.0,\n",
       "  'ent_coef': 0.01,\n",
       "  'clip_grad_norm': 1.0,\n",
       "  'use_standardization': 'True',\n",
       "  'lr_decay': 'False'},\n",
       " 'optim': {'name': 'adam', 'lr': 0.00025},\n",
       " 'train': {'training': 'True',\n",
       "  'load_path': './logs/drone_hanyang_mlagent/ppo/20230311094756544355/',\n",
       "  'run_step': 30000000,\n",
       "  'print_period': 10000,\n",
       "  'save_period': 500000,\n",
       "  'eval_iteration': 3,\n",
       "  'record': 'False',\n",
       "  'record_period': 1000000,\n",
       "  'distributed_batch_size': 256,\n",
       "  'update_period': \"agent['n_step']\",\n",
       "  'num_workers': 5}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(join(dir_model,'drone_hanyang_mlagent.ppo/drone_hanyang_mlagent.json'),'r') as f:\n",
    "    hanyang = json.load(f)\n",
    "hanyang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b11b2",
   "metadata": {},
   "source": [
    "## load pretrained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa73e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\n",
    "    join(dir_model,'drone_hanyang_mlagent.ppo/level2.ckpt')\n",
    "    , map_location=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87372585",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################\n",
      "network\n",
      "################\n",
      "head.conv1.weight\n",
      "torch.Size([32, 12, 8, 8])\n",
      "\n",
      "head.conv1.bias\n",
      "torch.Size([32])\n",
      "\n",
      "head.conv2.weight\n",
      "torch.Size([64, 32, 4, 4])\n",
      "\n",
      "head.conv2.bias\n",
      "torch.Size([64])\n",
      "\n",
      "head.conv3.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "\n",
      "head.conv3.bias\n",
      "torch.Size([64])\n",
      "\n",
      "l.weight\n",
      "torch.Size([512, 3840])\n",
      "\n",
      "l.bias\n",
      "torch.Size([512])\n",
      "\n",
      "mu.weight\n",
      "torch.Size([3, 512])\n",
      "\n",
      "mu.bias\n",
      "torch.Size([3])\n",
      "\n",
      "log_std.weight\n",
      "torch.Size([3, 512])\n",
      "\n",
      "log_std.bias\n",
      "torch.Size([3])\n",
      "\n",
      "v.weight\n",
      "torch.Size([1, 512])\n",
      "\n",
      "v.bias\n",
      "torch.Size([1])\n",
      "\n",
      "################\n",
      "optimizer\n",
      "################\n"
     ]
    }
   ],
   "source": [
    "for key1, dict_ in ckpt.items():\n",
    "    print('################')\n",
    "    print(key1)\n",
    "    print('################')\n",
    "    for key2, value in dict_.items():\n",
    "        if type(value) is not dict:\n",
    "            print('%s\\n%s\\n'%(key2,value.shape))\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83cd64f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a6a6cb",
   "metadata": {},
   "source": [
    "# Encoding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b157c0",
   "metadata": {},
   "source": [
    "#### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e76c273c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContinuousPolicyValue(\n",
       "  (head): CNN(\n",
       "    (conv1): Conv2d(12, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (l): Linear(in_features=3840, out_features=512, bias=True)\n",
       "  (mu): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (log_std): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (v): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.network import *\n",
    "\n",
    "InputShape = (12,72,128)\n",
    "network = policy_value.ContinuousPolicyValue(D_in=InputShape, D_out=3, D_hidden=512, head='cnn')\n",
    "network.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e746f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 12, 72, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 32, 17, 31]),\n",
       " torch.Size([5, 64, 7, 14]),\n",
       " torch.Size([5, 64, 5, 12]),\n",
       " torch.Size([5, 3]),\n",
       " torch.Size([5, 3]),\n",
       " torch.Size([5, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = torch.rand((5,*InputShape))\n",
    "print(input_.shape)\n",
    "(mu, std, v), (conv1, conv2, conv3) = network(input_.to(device))\n",
    "\n",
    "conv1.shape, conv2.shape, conv3.shape, mu.shape, std.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec5af4",
   "metadata": {},
   "source": [
    "#### copy parameters `network.head.weight = torch.nn.Parameter(ckpt['network'].head)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c25bc22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.load_state_dict(ckpt['network'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7bd87",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12f3b325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaling&#x27;, StandardScaler()), (&#x27;pca&#x27;, PCA(n_components=200))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaling&#x27;, StandardScaler()), (&#x27;pca&#x27;, PCA(n_components=200))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=200)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaling', StandardScaler()), ('pca', PCA(n_components=200))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 200\n",
    "pipeline_pca = Pipeline(\n",
    "    [\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('pca', PCA(n_components=n_components))\n",
    "    ]\n",
    ")\n",
    "pipeline_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25f6311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_forward(subj, run, shift=0):\n",
    "    ## ======================= setup ======================= ##\n",
    "    list_png, _, _ = get_data_behav(subj)\n",
    "    idx_input, idx_epi = get_idx_input(subj=subj, run=run, shift=shift)\n",
    "\n",
    "    for ii, (episode, indices) in enumerate(idx_epi.items()):\n",
    "        ## 해당 EPI index에 매칭되는 png의 index들\n",
    "        tmp = idx_input[episode]\n",
    "\n",
    "        ## 해당 RUN에 input으로 쓰일 모든 png fname 들\n",
    "        if ii > 0:\n",
    "            input_fname = np.concatenate(\n",
    "                [\n",
    "                    input_fname,\n",
    "                    list_png[run][tmp]\n",
    "                ],\n",
    "                axis=0\n",
    "            )\n",
    "        else:\n",
    "            input_fname = list_png[run][tmp]\n",
    "\n",
    "    ## (batch size of Run, 4)\n",
    "    (batch, _) = input_fname.shape\n",
    "\n",
    "    ## ======================= input ======================= ##\n",
    "    ## Actual input values to be entered into the network\n",
    "    input_ = np.zeros((batch,12,72,128), dtype=np.uint8)\n",
    "\n",
    "    for timepoint, fnames in enumerate(tqdm(input_fname)):\n",
    "        ## 4 frames (1,72,128,3) 를 numpy 로 불러와서 쌓음\n",
    "        tmp = np.stack(\n",
    "            [np.asarray(Image.open(f)).reshape((1,72,128,3)) for f in fnames],\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        ## 이미지 전처리 (4, 1, 72, 128, 3) -> (12, 72, 128, 3)\n",
    "        input_[timepoint] = state_processing(tmp)\n",
    "\n",
    "    del tmp\n",
    "    ## 이미지를 torch.tensor 로 변경\n",
    "    input_ = torch.from_numpy(input_)\n",
    "    ## ========================== Forward ========================== ##\n",
    "    ## 쌓은 frame 들을 network에 입력\n",
    "    with torch.no_grad():\n",
    "        output_ = network(input_.to(device))\n",
    "\n",
    "    return output_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679131ea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff416e3",
   "metadata": {},
   "source": [
    "#### Perform GLM (Generalized Linear Model) with the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8515dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_reg = join(dir_behav, 'regressors/AM/value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d68d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 230/1274 [00:03<00:16, 62.06it/s]"
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=6, figsize=(40,20))\n",
    "\n",
    "_, list_onsettime, list_episode = get_data_behav(subj)\n",
    "for rr, (run, onset) in enumerate(list_onsettime.items()):\n",
    "    ## onsettime\n",
    "    idx_input, idx_epi = get_idx_input(subj=subj, run=run, shift=0)\n",
    "    for ii, (key, val) in enumerate(idx_input.items()):\n",
    "        if ii > 0:\n",
    "            list_idx = np.concatenate([list_idx, val[:,0]])\n",
    "        else:\n",
    "            list_idx = val[:,0]\n",
    "    ## episode\n",
    "    episode = list_episode[run][list_idx]\n",
    "    \n",
    "    ## value function\n",
    "    (_,_,value),(_,_,_) = do_forward(subj, run, shift=0)\n",
    "    value = np.squeeze(value.numpy())\n",
    "    \n",
    "    ## plot\n",
    "    ax = axs[rr]\n",
    "    x = range(len(value))\n",
    "    ax.plot(x,value)\n",
    "    ylim = ax.get_ylim()\n",
    "    ax.grid(axis='y')\n",
    "    ax.fill_between(\n",
    "                x=x,\n",
    "                y1=0, y2=40,\n",
    "                where=(episode%2==0),\n",
    "                color='gray', alpha=0.5\n",
    "            )\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    for shift in list_shift:\n",
    "        ## make regressor\n",
    "        reg = []\n",
    "        for onset_, value_ in zip(onset[list_idx], value):\n",
    "            reg.append('%.2f*%.3f'%(onset_+shift,value_))\n",
    "        reg = np.array(reg)\n",
    "\n",
    "        # save the result\n",
    "        np.savetxt(\n",
    "            join(dir_reg, '%s.r%02d.value.shift=%1.1fs.txt'%(subj,rr+1,shift)),\n",
    "            X=reg,\n",
    "            fmt='%s', delimiter=' ', newline=' '\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7336dc5",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRN",
   "language": "python",
   "name": "drn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
