{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from glob import glob\n",
    "import os, sys\n",
    "from os.path import join, dirname\n",
    "from os.path import getsize\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "import nilearn.image, nilearn.masking\n",
    "from nilearn import plotting as nplt\n",
    "import nilearn.decoding\n",
    "\n",
    "import statsmodels.stats.multitest\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.decomposition import PCA\n",
    "# from PCRegression import PCR\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = ['GA', 'GB']\n",
    "nn_list = ['01', '02', '05', '07', '08', '11', '12', '13', '14', '15',\n",
    "           '18', '19', '20', '21', '23', '26', '27', '28', '29', '30',\n",
    "           '31', '32', '33', '34', '35', '36', '37', '38', '42', '44']\n",
    "run_list = ['r01', 'r02', 'r03', 'r04', 'r05', 'r06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/Volumes/T7SSD1/GA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fMRI_dir = root_dir + '/fMRI_data'\n",
    "preproc_dir = fMRI_dir + '/preproc_data'\n",
    "stat_dir = fMRI_dir + '/stats'\n",
    "roi_dir = fMRI_dir + '/roi'\n",
    "loc_dir = roi_dir + '/localizer'\n",
    "dmn_dir = roi_dir + '/DMN'\n",
    "\n",
    "data_dir = preproc_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_masking(img, roi):\n",
    "    # img : nifti image\n",
    "    # roi : nifti image\n",
    "    # output : (times, voxels)-dimension fdata array\n",
    "    img_data = img.get_fdata()\n",
    "    roi_mask = roi.get_fdata().astype(bool)\n",
    "    \n",
    "    if img_data.shape[:3] != roi_mask.shape:\n",
    "        raise ValueError('different shape while masking! img=%s and roi=%s' % (img_data.shape, roi_mask.shape))\n",
    "        \n",
    "    return img_data[roi_mask, :].T    # the shape is (times, voxels) which is to cross-validate for times(=runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA analysis\n",
    "model = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid(estimator, ROI_imgs, pickle_name):\n",
    "    # estimator : model\n",
    "    # ROI_imgs : nifti image\n",
    "    # pickle_name : string\n",
    "    # output : A leave-one-run-out cross-validation (LORO-CV) result and save pickle file\n",
    "    ## set the parameters\n",
    "    nrun = 3\n",
    "    ntpr = 96 # a number of trials per run\n",
    "    cv = GroupKFold(nrun)\n",
    "    y = [j for i in range(nrun) for j in target_pos] # answer\n",
    "    group = [i for i in range(nrun) for j in target_pos] # run number\n",
    "    \n",
    "    ## cross-validation\n",
    "    pkl = {}\n",
    "    for ii in id_list:\n",
    "        for nn in nn_list:\n",
    "            subj = ii + nn\n",
    "            for roi, roi_img in ROI_imgs.items():\n",
    "                for i, pp in enumerate(['practiced','unpracticed']):\n",
    "                    X = np.array([fast_masking(img=data[subj,rr],roi=roi_img) for rr in run_list[i*3:(i+1)*3]])\n",
    "                    X = X.reshape([nrun*ntpr,X.shape[-1]],order='C') # X.shape = (ntrial,nvoxle)\n",
    "                    score = cross_validate(\n",
    "                        estimator=estimator\n",
    "                        , X=X, y=y, groups=group\n",
    "                        , cv=cv, return_estimator=True, return_train_score=True)\n",
    "                    pkl[subj, roi, pp] = np.mean(score['test_score'])\n",
    "                print(subj, roi, end='\\r')\n",
    "                \n",
    "    ## Save as .pickle\n",
    "    with open(today+'_%s.pkl'%pickle_name,\"wb\") as fw:\n",
    "        pickle.dump(pkl, fw)\n",
    "        \n",
    "    return pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(roi_list, pkl):\n",
    "    wit_df = pd.DataFrame(columns=['subj', 'roi_name', 'Mapping', 'Mean Accuracy', 'Stage'])\n",
    "\n",
    "    for ii in id_list:\n",
    "        ss = 'Early' if ii == 'GA' else 'Late'\n",
    "        for nn in nn_list:\n",
    "            subj = ii + nn\n",
    "            for roi_name in roi_list:\n",
    "                for pp in ['unpracticed', 'practiced']:\n",
    "                    wit_df = wit_df.append(\n",
    "                        {'subj': subj\n",
    "                         ,'roi_name': roi_name\n",
    "                         ,'Mapping': pp\n",
    "                         ,'Mean Accuracy': np.mean(pkl[subj, roi_name, pp])\n",
    "                         ,'Stage': ss}\n",
    "                        , ignore_index=True)\n",
    "    return wit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wit_df_t_test(wit_df, roi_list, column_name, criteria):\n",
    "    gg = wit_df[column_name].unique()\n",
    "    key = list(criteria.keys())[0]\n",
    "    value = list(criteria.values())[0]\n",
    "    \n",
    "    ttest = {}\n",
    "    res = {}\n",
    "\n",
    "    for roi in roi_list:\n",
    "        sub_df = wit_df[(wit_df['roi_name'] == roi) & (wit_df[key] == value)]\n",
    "\n",
    "        mean_accs = [sub_df[sub_df[column_name] == i]['Mean Accuracy'] for i in gg]\n",
    "\n",
    "        ttest[roi] = scipy.stats.ttest_rel(mean_accs[0], mean_accs[1])\n",
    "        res[roi] = statsmodels.stats.multitest.fdrcorrection(ttest[roi].pvalue)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVPA\n",
    "#### Multi-voxel pattern analysis (MVPA) is gaining increasing interest in the neuroimaging community because it allows to detect differences between conditions with higher sensitivity than conventional univariate analysis by focusing on the analysis and comparison of distributed patterns of activity. In such a multivariate approach, data from individual voxels within a region are jointly analyzed. Furthermore, MVPA is often presented in the context of \"brain reading\" applications reporting that specific mental states or representational content can be decoded from fMRI activity patterns after performing a \"training\" or \"learning phase. In this context, MVPA tools are often referred to as classifiers or, more generally, learning machines. The latter names stress that many MVPA tools originate from a field called machine learning, a branch of artificial intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## background image\n",
    "img_bg = join(roi_dir,'mni152_2009bet.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM: overall fMRI activites associated with the 4 targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading $\\beta$s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load betas\n",
    "data = {}\n",
    "\n",
    "for ii in id_list:\n",
    "    for nn in nn_list:\n",
    "        subj = ii + nn\n",
    "        print(subj, end='\\r')\n",
    "        for run in run_list:\n",
    "            data[subj, run] = nilearn.image.load_img(join(data_dir,nn,'betasLSS.%s.%s.nii.gz'%(subj,run)))\n",
    "\n",
    "## indexing\n",
    "print('indexing...', end='\\r')\n",
    "for key, value in data.items():\n",
    "    data[key] = nilearn.image.index_img(value, np.arange(1, 97))\n",
    "\n",
    "## labeling with target position\n",
    "target_pos = []\n",
    "\n",
    "with open(join(root_dir,'targetID.txt')) as file:\n",
    "    for line in file:\n",
    "        target_pos.append(int(line.strip()))\n",
    "        \n",
    "target_pos = target_pos[1:97]\n",
    "target_path = list(range(1,13))*8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
